<!-- TEMPLATE: Review Skill
     This skill runs a "fresh-eyes" review. It MUST be run in a separate
     session from the one that did the original work. The whole point is
     that the reviewing AI has no memory of implementation decisions.

     Customize the review categories for your domain:
       SWE reviews: correctness, tests, security, consistency, tech debt
       EDA reviews: statistical validity, methodology, data handling, conclusions

     Replace {{ISSUE_LABEL}} with your task label.
     Replace {{REVIEW_CATEGORIES}} with your domain's review focus areas.
     Delete these comments when done.
-->
Run a fresh-eyes review. This must be run in a **separate session** from the one that did the original work.

The user specifies what to review: $ARGUMENTS
- Single task: `/review #42`
- Range: `/review #38 through #42`

---

## Step 1: Load Context

Read the project brief (`docs/{{BRIEF_DOC}}`), then read the issue(s) being reviewed from `issues/`.

For each issue, examine:
- The acceptance criteria
- The work products (files created or modified)
- The handoff note for that session

Do NOT read the design/plan from the original session — you are reviewing the output, not the process.

---

## Step 2: Evaluate

<!-- GUIDE: Replace these review categories with ones appropriate to your domain. -->

Evaluate the work against these criteria:

1. **Correctness** — Does the work actually accomplish what the acceptance criteria require?
2. **Quality** — Does the work meet professional standards for the domain?
3. **Completeness** — Is anything missing or partially done?
4. **Consistency** — Does the work fit with the rest of the project?
5. **Risks** — Does the work introduce any new risks or technical debt?

<!-- GUIDE: Add domain-specific review criteria here. For example:
     SWE adds: test coverage, security, performance
     EDA adds: statistical validity, methodology rigor, reproducibility -->

---

## Step 3: Categorize Findings

For each finding, categorize it:

- **Must Fix** — Incorrect behavior, broken acceptance criteria, or significant risk. Creates an issue in `issues/`.
- **Should Fix** — Quality concern, missing edge case, or inconsistency. Creates an issue in `issues/`.
- **Nit** — Style, naming, or minor improvement. Mentioned in the review summary but not tracked as an issue.

---

## Step 4: Create Issues for Findings

For each Must Fix and Should Fix finding, create an issue file in `issues/`:

Save to `issues/NNN-review-[short-slug].md`:

```markdown
# [Review] [Short description of finding]

**Label:** {{ISSUE_LABEL}}
**Status:** open

## User Story
As a [persona], I [need] [what this fix enables] so that [value].

## Description
**Found in:** issue [original issue number]
**Severity:** [Must Fix / Should Fix]

[Description of the finding and why it matters]

## Acceptance Criteria
- [ ] [What "fixed" looks like]
```

---

## Step 5: Summary

Present a summary to the user:

```
## Review Summary: #[issue(s)]

**Overall assessment:** [1-2 sentences]

### Must Fix ([count])
- #[new-issue] — [description]

### Should Fix ([count])
- #[new-issue] — [description]

### Nits ([count])
- [description — no issue created]
```

Rules:
- Be constructive, not harsh. The goal is to improve the work, not criticize.
- Review against the acceptance criteria, not your personal preferences.
- Don't suggest rewrites unless the current approach is fundamentally flawed.
- Findings go through a proper `/start` session — do not auto-fix.
